題號,難度,題目,選項 1,選項 2,選項 3,選項 4,正確答案,正確答案解說,錯誤答案解說
L3QA245,S,哪一個激活函數最容易導致梯度消失問題？,ReLU,Tanh,Sigmoid,Softmax,3,C. ? Sigmoid：容易在極端值趨近 0,"A. ReLU：可避免梯度消失，最常用
B. Tanh：仍可能出現梯度消失*
D. Softmax：多分類使用，不屬激活層"
L3QA246,S,深度學習中，哪一個步驟主要負責調整權重以降低損失？,前向傳播,激活函數,損失函數計算,反向傳播,4,D. ? 反向傳播：計算梯度並調整權重,"A. 前向傳播：只是計算預測結果
B. 激活函數：引入非線性
C. 損失函數：僅負責誤差度量"
L3QA247,S,下列哪一項是 CNN 中負責降低參數數量的技術？,Dropout,Padding,Pooling,Batch Normalization,3,C. ? Pooling：降低空間尺寸與參數量,"A. Dropout：防止過擬合
B. Padding：補零維持大小
D. BN：穩定訓練但不減參數"
L3QA248,S,哪一個不是 LSTM 的核心組件？,遺忘閘,記憶閘,輸出閘,輸入閘,2,B. ? 記憶閘：LSTM 沒有這個閘門,"A. 遺忘閘：控制保留記憶
C. 輸出閘：控制輸出內容
D. 輸入閘：控制新資訊"
L3QA249,S,哪個框架以「動態圖」建構為特色？,TensorFlow,Theano,PyTorch,MXNet,3,C. ? PyTorch：動態建圖，靈活即時,"A. TensorFlow：靜態圖為主
B. Theano：老牌靜態圖架構
D. MXNet：兩者並存"
L3QA250,S,以下哪個技術能有效減少過擬合？,激活函數,捲積層,Dropout,損失函數,3,"
C. ? Dropout：隨機忽略神經元","A. 激活函數：非為正則化技巧
B. 捲積層：主要用於特徵提取
D. 損失函數：衡量預測誤差"
L3QA251,S,GAN 的訓練包含兩個主要網路，分別是？,卷積網路與全連接網路,預測網與分析網,生成器與鑑別器,編碼器與解碼器,3,C. ? 生成器與鑑別器為核心,"A. 與 GAN 無關
B. 非常見稱呼
D. 為 autoencoder 結構"
L3QA252,S,Transformer 模型中，哪個機制使模型能捕捉長距離依賴？,Pooling,Self-Attention,Dropout,全連接層,2,B. ? Self-Attention：可關注任意位置,"A. Pooling：降維處理
C. Dropout：正則化非注意力
D. FC 層：線性轉換"
L3QA253,S,下列哪個是「前向傳播」的主要任務？,計算梯度,更新參數,預測輸出,測試準確率,3,C. ? 負責從輸入計算預測結果,"A. 梯度計算屬反向傳播
B. 參數更新屬於 optimizer
D. 準確率為評估階段"
L3QA254,S,在 Keras 中訓練模型時，哪一個函數負責開始訓練？,compile(),fit(),predict(),evaluate(),2,B. ? fit：啟動訓練流程,"A. compile：設定損失與優化器
C. predict：產生預測結果
D. evaluate：評估效能"
L3QA255,S,一個深度神經網路在訓練時 loss 不斷下降，但驗證集 loss 開始上升，此時最佳處理方式是？,啟用 Early Stopping,增加學習率,使用更大 batch size,改用 Softmax 激活函數,1,A. ? Early Stopping 可避免 overfitting,"B. 增加學習率可能導致發散
C. 更大 batch 不一定改善泛化
D. Softmax 與過擬合無直接關係"
L3QA256,S,下列哪一項不是 Transformer 編碼器的基本組件？,多頭注意力機制,前饋神經網路,遞迴層（RNN）,殘差連接,3,C. ? Transformer 不使用 RNN 結構,"A. 多頭注意力是核心
B. 前饋層增強特徵表達
D. 殘差結構避免梯度消失"
L3QA257,S,在 LSTM 訓練時發生梯度爆炸，最佳的處理方式為？,使用 ReLU 激活函數,減少時間步數,加入 Batch Normalization,啟用 Gradient Clipping,4,D. ? Gradient Clipping 可限制梯度,"A. ReLU 對 LSTM 效果有限
B. 時間步短未必解決
C. BN 較少用於 RNN"
L3QA258,S,下列哪個敘述正確描述 CNN 的反向傳播？,池化層不傳遞梯度,濾波器的梯度透過輸入與誤差卷積取得,反向傳播不涉及偏差項,Max Pooling 使用平均傳回法,2,B. ? 透過誤差與輸入進行卷積得梯度,"A. 錯，Max Pooling 有選擇性梯度回傳
C. 偏差項也需更新
D. Max 非平均傳回"
L3QA259,S,下列哪個選項最能描述 Batch Normalization 的作用？,穩定中間層的輸出分布,增加模型容量,避免過多參數更新,加快資料讀取速度,1,A. ? 使中間層輸出更穩定，加速收斂,"B. 模型容量由層數與參數決定
C. 與更新次數無關
D. 與 I/O 無關"
L3QA260,S,若模型在訓練時發生梯度消失問題，以下哪種做法最可能有效？,增加學習率,改用 ReLU 激活函數,加入 Dropout,增加損失函數項數,2,B. ? ReLU 不易造成梯度消失,"A. 太大學習率可能導致不穩
C. Dropout 防過擬合非解梯度問題
D. 損失增加無關"
L3QA261,S,在深度學習中使用 Softmax 時，若模型預測結果極端不平均，可能的原因為？,學習率過高,過擬合,輸出未正規化,隱藏層太多,3,C. ? 未正規化會導致不穩定,"A. 不會直接造成 Softmax 極值
B. 不一定造成極端機率
D. 層數多不等於極端輸出"
L3QA262,S,哪個框架特別適合部署於多平台設備並提供模型轉換功能？,Keras,PyTorch,TensorFlow,ONNX,4,D. ? ONNX 為模型交換格式,"A. Keras 適用於快速開發
B. PyTorch 偏向研究
C. TensorFlow 雖能部署但較封閉"
L3QA263,S,若你需要一個具備雙向序列處理能力的模型，最佳選擇為？,普通 LSTM,單向 GRU,雙向 RNN（Bi-RNN）,多層 CNN,3,C. ? Bi-RNN 能雙向理解序列,"A. 普通 LSTM 只看過去
B. 單向 GRU 同樣不處理未來
D. CNN 雖可捕捉區域資訊，非時間序列擅長"
L3QA264,S,若一個模型過於複雜、在小資料集上表現不穩定，最適合的處理方式為？,減少模型參數或層數,提升學習率,使用更多激活函數,加大訓練次數,1,A. ? 降低複雜度可防止過擬合,"B. 過高學習率不穩
C. 非常規做法
D. 可能加劇 overfitting"
L3QA265,S,以下哪個優化方法主要使用「一階導數」進行參數更新？,牛頓法,Lagrange 乘子法,梯度下降法,遺傳演算法,3,C. ? 只使用一階導數（梯度）更新參數,"A. 使用一階與二階導數（非純一階）
B. 處理限制條件問題
D. 隨機啟發式，不用導數"
L3QA266,S,下列何者不是 Adam 優化器的特點？,自動調整學習率,結合 Momentum 與 RMSProp,計算二階導數以求極小值,適用於深度學習模型訓練,3,C. ? Adam 是一階方法，不需二階導數,"A. Adam 根據梯度變化自動調整學習率
B. 結合了動量與 RMSProp 的特性
D. 常見於深度學習任務"
L3QA267,S,下面哪個技術是用來避免訓練過度（Overfitting）的方法？,梯度上升,Early Stopping,增加學習率,使用固定學習率,2,B. ? 驗證 loss 上升即停止訓練,"A. 最大化函數，不針對 overfitting
C. 易導致 loss 發散
D. 無法防止過擬合"
L3QA268,S,在梯度下降中，學習率 α 太小會造成什麼結果？,發散,訓練過度,收斂緩慢,無法計算梯度,3,C. ? 步伐太小導致進度緩慢,"A. 發散通常來自學習率過大
B. 與過擬合無關
D. 與學習率無關"
L3QA269,S,RMSProp 的優勢在於哪一點？,計算精確的二階導數,適合處理非平穩目標,不需要學習率參數,適用於決策樹訓練,2,B. ? 可動態調整步伐，適合如 RNN 等非平穩目標,"A. 非二階方法
C. 仍需設定學習率
D. 決策樹不使用此法"
L3QA270,S,牛頓法與梯度下降法的主要差異在於？,牛頓法適用於大型資料集,牛頓法需計算 Hessian 矩陣,梯度下降法比牛頓法快,梯度下降法無需計算梯度,2,B. ? 牛頓法使用二階導數（Hessian）,"A. 牛頓法成本高，不適合大型資料
C. 牛頓法理論收斂較快
D. 錯誤，需計算梯度"
L3QA271,S,在神經網路訓練中，學習率過大會導致什麼現象？,模型無法記住訓練資料,模型 loss 持續震盪甚至發散,模型收斂到最佳值,訓練過程穩定但緩慢,2,B. ? 步伐過大導致不穩定,"A. 與記憶力無關
C. 學習率需適中才收斂
D. 是 α 太小的表現"
L3QA272,S,Adam Optimizer 中 β1 的角色為何？,控制學習率衰減速度,平滑一階動量（平均梯度）,調整激活函數形狀,提高梯度爆炸風險,2,B. ? 控制一階動量的衰減率,"A. 與 learning rate schedule 無關
C. 無關
D. 實際上可避免爆炸"
L3QA273,S,為何 L-BFGS 常用於小樣本模型？,快速處理非凸問題,高效逼近二階導數,無需記憶體資源,專為深度學習設計,2,B. ? 是近似二階導數法，適合精準小樣本,"A. 非凸問題不穩定
C. 仍需儲存歷史資訊
D. 不適用大型深度網路"
L3QA274,S,下列哪個不是學習率調整策略？,Step Decay,ReduceLROnPlateau,Gradient Normalization,Cyclical Learning Rate,3,"C. ? 是處理梯度爆炸的技術
","A. 每隔幾個 epoch 降低學習率
B. 根據驗證 loss 自調 α
D. 在上下限間擺動學習率"
L3QA275,S,你在訓練一個深度學習模型，loss 起初快速下降，後期停滯。此時最佳策略為？,增加學習率,使用梯度裁剪,套用 Learning Rate Decay,更換啟用函數,3,C. ? 降學習率有助後期收斂,"A. 易發散
B. 解梯度爆炸非 loss 停滯
D. 啟用函數非主因"
L3QA276,S,將 Adam 的 β1 調至 0.999，最可能導致？,模型訓練速度變快,更新方向變得敏感,動量過於平滑導致反應變慢,學習率不再需要調整,3,"C. ? 平滑過度導致慢反應
","A. 越大越慢
B. 越小才更敏感
D. 仍需設學習率"
L3QA277,S,若目標函數有預算與人力限制，應用哪種優化方法？,梯度下降,牛頓法,Lagrange 乘子法,RMSProp,3,C. ? 處理限制問題首選,"A. 無限制式才可
B. 非處理限制式
D. 非約束問題適用"
L3QA278,S,下列關於 Adam 的敘述何者錯誤？,可視為 RMSProp 與 Momentum 融合,需設計 β1、β2,屬於二階優化法,對新手友善,3,C. ? 錯，屬於一階方法,"A. 正確
B. 正確
D. 預設值穩定"
L3QA279,S,若梯度為 0.8、學習率為 0.05、平方梯度平均為 0.64，RMSProp 的 denominator 約為？,0.05,0.8,0.25,0.8 / sqrt(0.64 + 1e?8),4,D. ? 符合 RMSProp 更新式,"A. 應取平方根後運算
B. 為梯度非分母
C. 計算不符"
L3QA280,S,哪種情境最適合使用 L-BFGS？,訓練上百萬筆資料,小樣本精準參數優化任務,RNN 預測,強化學習策略更新,2,B. ? 近似二階法，適合小資料,"A. 不適合大模型
C. 通常用 RMSProp
D. 多用策略梯度法"
L3QA281,S,若 loss 曲線先降後震盪再微升，最合理的解釋為？,學習率過低,初始化太小,陷入局部極小值,出現 overfitting,4,D. ? 訓練過度,"A. 不會 loss 上升
B. 與 loss 上升無關
C. 應該平穩不上升"
L3QA282,S,模型 loss curve 停滯，最佳策略為？,提高學習率,套用 ReduceLROnPlateau,改用 SGD,移除正則化,2,B. ? 根據驗證 loss 自動降 α,"A. 易震盪
C. 效率低
D. 易 overfit"
L3QA283,S,優化問題含不等式限制（如 x?0），適合哪個條件？,牛頓收斂條件,KKT 條件,RMS 條件,拋物線逼近法則,2,B. ? KKT 處理不等式限制,"A. 是算法條件非最佳性
C. 無此條件
D. 為擬合技巧"
L3QA284,S,使用固定學習率時，loss 後期震盪，最佳策略為？,提高 momentum,改用梯度上升,使用 learning rate decay,改 sigmoid 激活,3,C. ? 可調整步伐改善震盪,"A. 效果有限
B. 與最小化目標衝突
D. activation 改變非核心解法"
