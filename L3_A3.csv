題號,難度,題目,選項 1,選項 2,選項 3,選項 4,正確答案,正確答案解說,錯誤答案解說
L3QA245,S,哪一個激活函數最容易導致梯度消失問題？,ReLU,Tanh,Sigmoid,Softmax,3,C. ? Sigmoid：容易在極端值趨近 0,A. ReLU：可避免梯度消失，最常用\nB. Tanh：仍可能出現梯度消失*\nD. Softmax：多分類使用，不屬激活層
L3QA246,S,深度學習中，哪一個步驟主要負責調整權重以降低損失？,前向傳播,激活函數,損失函數計算,反向傳播,4,D. ? 反向傳播：計算梯度並調整權重,A. 前向傳播：只是計算預測結果\nB. 激活函數：引入非線性\nC. 損失函數：僅負責誤差度量
L3QA247,S,下列哪一項是 CNN 中負責降低參數數量的技術？,Dropout,Padding,Pooling,Batch Normalization,3,C. ? Pooling：降低空間尺寸與參數量,A. Dropout：防止過擬合\nB. Padding：補零維持大小\nD. BN：穩定訓練但不減參數
L3QA248,S,哪一個不是 LSTM 的核心組件？,遺忘閘,記憶閘,輸出閘,輸入閘,2,B. ? 記憶閘：LSTM 沒有這個閘門,A. 遺忘閘：控制保留記憶\nC. 輸出閘：控制輸出內容\nD. 輸入閘：控制新資訊
L3QA249,S,下列哪一個是處理長序列依賴問題的關鍵機制？,CNN,RNN,Attention Mechanism,Fully Connected Layer,3,C. ? 注意力機制：可捕捉長距離依賴,A. CNN：擅長局部特徵擷取\nB. RNN：雖可處理序列但長依賴效果差\nD. 全連通層：不處理序列關係
L3QA250,S,在機器學習中，下列哪一個指標最適合用來衡量模型對於**類別不平衡**資料集的表現？,準確率（Accuracy）,均方誤差（MSE）,F1 Score,R-squared,3,C. ? F1 Score：綜合考量精確率與召回率，適合不平衡資料,"A. 準確率：在高不平衡資料下參考價值低\nB. MSE, D. R-squared：適用於迴歸問題"
L3QA251,S,在影像辨識中，Padding 的主要目的為何？,增加模型訓練速度,防止梯度爆炸,確保卷積運算後輸出維度與輸入維度一致,降低模型的複雜度,3,C. ? Padding 補零是為確保邊緣資訊不遺失且維持尺寸，避免圖像邊緣資訊被卷積層丟棄，確保輸出尺寸與輸入一致。,A. 不能加速\nB. 不防梯度爆炸\nD. 不會降低複雜度
L3QA252,S,Batch Normalization (BN) 最主要的用途是？,防止資料洩露,加速模型收斂並穩定訓練,取代激活函數,大幅減少模型參數,2,B. ? BN：使各層輸入分佈穩定，解決 Internal Covariate Shift 問題，加速收斂。,A. 與資料洩露無關\nC. 不能取代激活函數\nD. 不減少參數
L3QA253,S,若模型在訓練集表現優異，但在測試集表現差，最可能的原因是？,欠擬合（Underfitting）,過擬合（Overfitting）,梯度消失,Batch Size 過小,2,B. ? 過擬合：表示模型記憶了訓練集的雜訊，泛化能力差。,A. 欠擬合：訓練/測試集皆差\nC. 梯度消失：會導致訓練不佳\nD. Batch Size 過小：可能導致訓練不穩
L3QA254,S,下列哪一項不是常見的 L2 Regularization (權重衰減) 目的？,限制權重大小,防止過擬合,簡化模型,將部分權重強制設為零,4,D. ? L2 懲罰大權重，但不強制設為零 (這是 L1 的特性)。,"A, B, C. L2 透過懲罰大權重來限制模型複雜度、防止過擬合"
L3QA255,S,下列哪一個優化器（Optimizer）最適合處理稀疏梯度（Sparse Gradients）的資料？,SGD,Adagrad,Adam,RMSProp,2,B. ? Adagrad：可根據參數過去的梯度調整學習率，適合稀疏資料，但缺點是學習率可能遞減過快。,"A. SGD：無自適應能力\nC, D. Adam/RMSProp：雖優於 Adagrad 但在稀疏梯度下，Adagrad 具備專門的優勢。"
L3QA256,S,在訓練神經網路時，設定較高的 Learning Rate (學習率) 最可能導致的後果是？,模型收斂速度變慢,陷入局部最小值,訓練過程發散,訓練時間縮短,3,C. ? 學習率過高易導致參數更新幅度過大，跨越最優點，使訓練過程發散。,A. 學習率高會加速收斂，但有發散風險\nB. 高學習率有助跳出局部最小值\nD. 訓練時間與收斂性有關，不必然縮短
L3QA257,S,下列哪一項是生成式 AI 模型中，衡量**輸出多樣性**的超參數？,Learning Rate,Batch Size,Temperature,Weight Decay,3,C. ? Temperature：調整 Softmax 函數的分佈，值越高，輸出越隨機多樣。,A. 學習率：影響訓練收斂\nB. Batch Size：影響梯度計算\nD. Weight Decay：L2 正則項
L3QA258,S,在 CNN 訓練過程中，Dropout Layer 的作用是？,增加模型的深度,隨機捨棄部分神經元以防止過擬合,將高維特徵降維,加快前向傳播速度,2,B. ? Dropout：隨機暫時丟棄神經元，避免神經元間的共適應，是常見的防過擬合技術。,"A, C, D. 與深度、維度、速度無關"
L3QA259,S,訓練一個二元分類模型時，若希望同時評估模型的**精確率 (Precision) 與召回率 (Recall)**，應使用哪個損失函數或指標？,交叉熵損失（Cross-Entropy Loss）,均方誤差（MSE）,F1 Score,R-squared,3,C. ? F1 Score：是 Precision 與 Recall 的調和平均，適合綜合評估。,"A. 交叉熵：損失函數\nB. MSE, D. R-squared：迴歸指標"
L3QA260,S,若想在深度學習模型中引入**非線性**能力，應使用下列哪一項組件？,線性層（Linear Layer）,激活函數（Activation Function）,損失函數（Loss Function）,優化器（Optimizer）,2,B. ? 激活函數：將非線性因素引入模型，使神經網路能夠逼近任何複雜函數。,A. 線性層：僅做線性變換\nC. 損失函數：度量誤差\nD. 優化器：調整權重
L3QA261,S,下列哪一項是 Transformer 模型結構中，相較於 RNN 的主要優勢？,計算複雜度較低,無法處理長序列,支援並行計算,需要較少的訓練資料,3,C. ? Attention 機制使 Transformer 能同時處理序列所有位置，支持並行計算，大幅加快訓練速度。,A. Transformer 計算複雜度較高\nB. Transformer 更擅長處理長序列\nD. 需要的資料量通常較大
L3QA262,S,在模型訓練中，**早停法（Early Stopping）**的主要目的是？,避免梯度爆炸,選擇最佳學習率,防止過擬合,加速模型推論,3,C. ? 早停法：監控驗證集（Validation Set）的損失，當損失不再下降時停止訓練，防止模型繼續過擬合訓練集。,A. 不防梯度爆炸\nB. 不選學習率\nD. 不加速推論
L3QA263,S,在 CNN 架構中，卷積核（Kernel）的深度（Depth）應等於？,輸入特徵圖的寬度,輸入特徵圖的高度,輸入特徵圖的通道數,輸出特徵圖的通道數,3,C. ? 卷積核的深度必須等於輸入特徵圖的通道數（如 RGB 圖像的通道數為 3）。,"A, B, D. 與寬度、高度、輸出通道數無關"
L3QA264,S,使用 K-fold Cross-Validation (K-折交叉驗證) 的主要目的為何？,加快模型訓練速度,減少資料標註成本,更穩定地評估模型的泛化能力,更容易進行特徵工程,3,C. ? K-fold 透過多次劃分訓練/驗證集，能更全面且穩定地評估模型對未見資料的泛化能力。,"A, B, D. 與速度、成本、特徵工程無關"
L3QA265,S,哪一種損失函數最適合用於多分類問題？,均方誤差（MSE）,交叉熵損失（Cross-Entropy Loss）,Hinge Loss,L1 Loss,2,B. ? 交叉熵（Cross-Entropy）：專門用於分類問題，特別是多分類。,"A, D. 均方誤差/L1：適用於迴歸\nC. Hinge Loss：適用於 SVM 或二元分類"
L3QA266,S,在訓練 GAN（生成對抗網路）時，下列哪一項敘述是錯誤的？,生成器（Generator）和判別器（Discriminator）是同時訓練的,目標是讓判別器無法區分真實與生成數據,生成器只關注生成逼真的資料,判別器只關注區分數據的真偽,3,C. ? 生成器目標是讓判別器無法區分，而不僅是生成逼真的資料。,A. GAN 為對抗訓練\nB. 判別器無法區分表示生成器成功\nD. 判別器是二元分類器
L3QA267,S,下列哪一項技術是**有損壓縮（Lossy Compression）**的代表？,PCA,One-Hot Encoding,Bagging,Boosting,1,A. ? PCA 透過捨棄低方差（低資訊）的主成分，故為有損壓縮。,"B. One-Hot：編碼\nC, D. Bagging/Boosting：集成學習"
L3QA268,S,相較於傳統機器學習，深度學習（Deep Learning）的核心優勢是？,模型可解釋性高,能夠自動從原始數據中提取特徵,訓練速度快,需要的訓練資料量較少,2,B. ? 深度學習（尤其是 CNN; RNN; Transformer）能自動學習（提取）特徵，無需人工設計特徵工程。,"A, C, D. 可解釋性低、訓練速度慢、需資料量大是常見缺點"
L3QA269,S,在深度學習中，梯度爆炸（Gradient Exploding）的解決方案通常不包括下列哪一項？,使用 ReLU 激活函數,使用梯度裁剪（Gradient Clipping）,減小學習率,使用 Batch Normalization,1,A. ? 使用 ReLU 旨在解決梯度消失，而非爆炸。,B. 梯度裁剪：直接限制梯度大小\nC. 減小學習率：減小參數更新幅度\nD. BN：能規範各層輸入，穩定梯度
L3QA270,S,下列哪一項不是防止過擬合（Overfitting）的常見方法？,增加訓練資料量,使用 L1/L2 正則化,使用 Dropout,增加模型複雜度,4,D. ? 增加模型複雜度會**加劇**過擬合，應當是減少模型複雜度或簡化模型。,"A, B, C. 皆為防止過擬合的常見手段"
L3QA271,S,在自然語言處理（NLP）中，哪一個模型架構最常被用於**序列到序列（Seq2Seq）**任務，如機器翻譯？,CNN,Autoencoder,Encoder-Decoder with Attention,HMM,3,C. ? 帶有 Attention 機制的 Encoder-Decoder 架構是 Seq2Seq 任務的經典模型（如 Transformer 基礎）。,A. CNN：常用於文本分類\nB. Autoencoder：常用於降維或特徵提取\nD. HMM：傳統機器學習模型
L3QA272,S,下列哪一個優化器（Optimizer）結合了動量（Momentum）與自適應學習率（Adaptive Learning Rate）？,SGD,Adagrad,RMSProp,Adam,4,D. ? Adam (Adaptive Moment Estimation) 結合了 Momentum (一階矩) 與 RMSProp (二階矩) 的優勢。,A. SGD：無自適應能力\nB. Adagrad：只有自適應學習率\nC. RMSProp：只有自適應學習率
L3QA273,S,若一個神經網絡的訓練集與測試集損失皆高，代表發生了什麼問題？,過擬合（Overfitting）,欠擬合（Underfitting）,梯度爆炸,數據洩漏（Data Leakage）,2,B. ? 兩者損失皆高表示模型學習不足，無法捕捉資料規律，即欠擬合。,A. 過擬合：訓練損失低，測試損失高\nC. 梯度爆炸：會導致訓練不穩定，不一定造成損失皆高\nD. 數據洩漏：結果難以預測
L3QA274,S,下列哪個不是學習率調整策略？,Step Decay,ReduceLROnPlateau,Gradient Normalization,Cyclical Learning Rate,3,C. ? Gradient Normalization 是處理梯度爆炸的技術，非學習率調整策略。,A. 每隔幾個 epoch 降低學習率\nB. 根據驗證 loss 自調 α\nD. 在上下限間擺動學習率
L3QA275,S,你在訓練一個深度學習模型，loss 起初快速下降，後期停滯。此時最佳策略為？,增加學習率,使用梯度裁剪,套用 Learning Rate Decay,更換啟用函數,3,C. ? 降學習率有助後期收斂，可以讓模型更精確地找到最優點。,A. 易發散\nB. 解梯度爆炸非 loss 停滯\nD. 啟用函數非主因
L3QA276,S,將 Adam 的 $\beta_1$ 調至 0.999，最可能導致？,模型訓練速度變快,更新方向變得敏感,動量過於平滑導致反應變慢,學習率不再需要調整,3,C. ? $\beta_1$ 接近 1 意味著過去的動量權重極高，平滑過度會導致模型對當前梯度反應變慢。,A. 越大越慢\nB. 越小才更敏感\nD. 仍需設學習率
L3QA277,S,若目標函數有預算與人力限制，應用哪種優化方法？,梯度下降,牛頓法,Lagrange 乘子法,RMSProp,3,C. ? Lagrange 乘子法專門用於處理帶有約束條件（限制式）的最佳化問題。,A. 無限制式才可\nB. 非處理限制式\nD. 非約束問題適用
L3QA278,S,下列關於 Adam 的敘述何者錯誤？,使用一階矩（Momentum）和二階矩（RMSProp）的移動平均,每次迭代的學習率都是固定的,在稀疏梯度問題上表現優秀,需要調整 $\beta_1$ 和 $\beta_2$ 兩個超參數,2,B. ? Adam 屬於自適應學習率優化器，學習率是動態調整而非固定的。,"A, C, D. 皆為 Adam 優點或參數"
L3QA279,S,若梯度為 0.8、學習率為 0.05、平方梯度平均為 0.64，RMSProp 的 denominator 約為？,0.05,0.8,0.25,0.8 / sqrt(0.64 + 1e?8),4,D. ? 符合 RMSProp 更新式,A. 應取平方根後運算\nB. 為梯度非分母\nC. 計算不符
L3QA280,S,哪種情境最適合使用 L-BFGS？,訓練上百萬筆資料,小樣本精準參數優化任務,RNN 預測,強化學習策略更新,2,B. ? 近似二階法，適合小資料,A. 不適合大模型\nC. 通常用 RMSProp\nD. 多用策略梯度法
L3QA281,S,若 loss 曲線先降後震盪再微升，最合理的解釋為？,學習率過低,初始化太小,陷入局部極小值,出現 overfitting,4,D. ? 訓練過度,A. 不會 loss 上升\nB. 與 loss 上升無關\nC. 應該平穩不上升
L3QA282,S,模型 loss curve 停滯，最佳策略為？,提高學習率,套用 ReduceLROnPlateau,改用 SGD,移除正則化,2,B. ? 根據驗證 loss 自動降 α,A. 易震盪\nC. 效率低\nD. 易 overfit
L3QA283,S,優化問題含不等式限制（如 x?0），適合哪個條件？,牛頓收斂條件,KKT 條件,RMS 條件,拋物線逼近法則,2,B. ? KKT 處理不等式限制,A. 是算法條件非最佳性\nC. 無此條件\nD. 為擬合技巧
L3QA284,S,使用固定學習率時，loss 後期震盪，最佳策略為？,提高 momentum,改用梯度上升,使用 learning rate decay,改 sigmoid 激活,3,C. ? 可調整步伐改善震盪,A. 效果有限\nB. 與最小化目標衝突\nD. activation 改變非核心解法
